include required(classpath("application"))
system.io {
  # For our shared cluster, you want to hit it with lots of requests and let SLURM figure out priority, rather than waiting.
  number-of-requests = 1000000
  per = 1 seconds
  number-of-attempts = 5
}
system {
   file-hash-cache = true
   # Sometimes the defaults for input read limits were too small. These increase the max file sizes.
   input-read-limits {
       tsv = 1073741823
       object = 1073741823
       string = 1073741823
       lines = 1073741823
       json = 1073741823
   }
}
database {
  profile = "slick.jdbc.MySQLProfile$"
  db {
    driver = "com.mysql.cj.jdbc.Driver"
    connectionTimeout = 12000
  }
}
workflow-options {
    # save all workflow logs to refer back to
    workflow-log-temporary = false
}
akka.http.server.request-timeout = 30s
call-caching {
  # Allows re-use of existing results for jobs you've already run
  # (default: false)
  enabled = true

  # Whether to invalidate a cache result forever if we cannot reuse them. Disable this if you expect some cache copies
  # to fail for external reasons which should not invalidate the cache (e.g. auth differences between users):
  # (default: true)
  invalidate-bad-cache-results = true
}

### Backend and filesystem
backend {
  default = SLURM
  providers {
    SLURM {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        runtime-attributes = """
        Int cpu = 1
        Int memory_mb = 2000
        String walltime = "18:00:00"
        String partition = "campus"
        String cluster = "gizmo"
        String? docker
        String? modules = ""
        """

        submit = """
            set -e 
            source /app/Lmod/lmod/lmod/init/bash
            module use /app/easybuild/modules/all
            module purge

            module load ${modules}
            
            sbatch \
              --wait \
              --partition=${partition} \
              --clusters=${cluster} \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              --cpus-per-task=${cpu} \
              --mem=${memory_mb} \
              --time=${walltime} \
              --wrap "/bin/bash ${script}"
        """
        submit-docker = """
            set -e 
            # Ensure singularity is loaded if it's installed as a module
            module load Singularity/3.5.3

            # Submit the script to SLURM
            sbatch \
              --wait \
              --partition=${partition} \
              --clusters=${cluster} \
              -J ${job_name} \
              -D ${cwd} \
              -o ${cwd}/execution/stdout \
              -e ${cwd}/execution/stderr \
              --cpus-per-task=${cpu} \
              --mem=${memory_mb} \
              --time=${walltime} \
              --wrap "singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"

        """
       filesystems {
          local {
            localization: [
              ## for local SLURM, hardlink doesn't work. Options for this and caching: , "soft-link" , "hard-link", "copy"
              "soft-link", "copy"
            ]
            ## call caching config relating to the filesystem side
            caching {
              # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:
              duplication-strategy: [
                "soft-link"
              ]
              hashing-strategy: "path+modtime"
              # Possible values: file, path, path+modtime
              # "file" will compute an md5 hash of the file content.
              # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
              # in order to allow for the original file path to be hashed.

              check-sibling-md5: false
              # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
              # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
            }
          }
        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+).*"
      }
    }
  }
}
